{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_class_weight() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Calculer les poids de classe pour la pondération\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m compute_class_weight(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(y_train), y_train)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Créer les modèles avec les options spécifiques pour gérer le déséquilibre de classe\u001b[39;00m\n\u001b[0;32m     28\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: LogisticRegression(class_weight\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m: class_weights[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m: class_weights[\u001b[38;5;241m1\u001b[39m]}),\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: RandomForestClassifier(class_weight\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m: class_weights[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m: class_weights[\u001b[38;5;241m1\u001b[39m]}),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoost\u001b[39m\u001b[38;5;124m'\u001b[39m: CatBoostClassifier(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, class_weights\u001b[38;5;241m=\u001b[39mclass_weights)  \u001b[38;5;66;03m# Utilisation de la pondération de classe\u001b[39;00m\n\u001b[0;32m     38\u001b[0m }\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_class_weight() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Charger les données\n",
    "data = pd.read_excel(\"dataset_new.xlsx\")\n",
    "\n",
    "# Séparer les caractéristiques (features) et la variable cible\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculer les poids de classe pour la pondération\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "\n",
    "# Créer les modèles avec les options spécifiques pour gérer le déséquilibre de classe\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight={0: class_weights[0], 1: class_weights[1]}),\n",
    "    'Random Forest': RandomForestClassifier(class_weight={0: class_weights[0], 1: class_weights[1]}),\n",
    "    'Support Vector Machine': SVC(probability=True),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'XGBoost': XGBClassifier(scale_pos_weight=class_weights[1]),  # Utilisation du paramètre scale_pos_weight\n",
    "    'LightGBM': LGBMClassifier(class_weight={0: class_weights[0], 1: class_weights[1]}),  # Pondération de classe\n",
    "    'CatBoost': CatBoostClassifier(verbose=0, class_weights=class_weights)  # Utilisation de la pondération de classe\n",
    "}\n",
    "\n",
    "# Définir les paramètres pour GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Définir la stratégie de cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Entraîner et évaluer les modèles avec GridSearchCV\n",
    "for model_name, model in models.items():\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'{model_name} - Best params: {grid_search.best_params_}')\n",
    "    print(f'{model_name} - Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Matrice de Confusion\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    print(f'{model_name} - Confusion Matrix:\\n{confusion_mat}')\n",
    "    \n",
    "    # Précision, Recall, F1-Score\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f'{model_name} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n",
    "    \n",
    "    # AUC-ROC\n",
    "    if hasattr(best_model, 'predict_proba'):\n",
    "        y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "        auc_roc = roc_auc_score(y_test, y_pred_prob)\n",
    "        print(f'{model_name} - AUC-ROC: {auc_roc:.4f}')\n",
    "    \n",
    "    print('\\n')  # Ajout d'une ligne vide entre les résultats de différents modèles\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
